# RUMC 创新点详细整理

---

## 1. MC Dropout 随机采样与不确定性估计

- **原理**：在生成分子时，模型切换到训练模式（启用 Dropout），对同一输入多次采样，得到多个分子及其奖励分布。
- **统计方法**：对每个分子，统计其奖励均值和标准差，计算 UCB（Upper Confidence Bound）分数：  
  `UCB = mean_reward + λ * std_reward / sqrt(count)`，用于平衡探索与利用。
- **优势**：能够发现奖励高但模型不确定性大的分子，提升生成分子的多样性和潜在质量。
- **代码位置**：`GenSampler.sample_with_uncertainty` 方法。

---

## 2. 奖励归一化机制

- **原理**：每次分子采样后，奖励会被归一化（减去均值，除以标准差），使奖励分布更稳定。
- **实现细节**：
  - 归一化奖励用于经验池筛选和采样概率计算，避免奖励分布偏移导致训练不稳定。
  - 归一化后，经验池优先保留高归一化奖励分子。
- **代码位置**：`RewardBuffer.normalize_reward` 方法及相关逻辑。

---

## 3. 经验池（Replay Buffer）构建与智能回放

- **原理**：维护一个分子经验池，存储历史采样中奖励较高的分子，自动去重，优先保留高奖励分子。
- **采样机制**：
  - 采样时可按概率（如 0.3）从经验池中选取分子作为条件生成起点，实现经验回放。
  - 经验池采样采用奖励加权的 softmax 分布，提升高奖励分子的采样概率。
  - 经验池满时，低奖励分子会被高奖励分子替换。
- **优势**：提升生成质量，增强模型对高奖励分子的记忆和利用，促进探索与 exploitation 的平衡。
- **代码位置**：`RewardBuffer` 类及 `GenSampler.sample` 方法。

---

## 4. 采样与奖励驱动生成流程

- **采样流程**：
  1. 生成分子序列（SMILES），可选用经验池样本作为起点。
  2. 对生成分子进行奖励评估（如药物性评分）。
  3. 奖励归一化后，分子和奖励存入经验池。
  4. 可通过 MC Dropout 多次采样，统计分子奖励均值和不确定性，筛选高 UCB 分子。
- **高奖励分子筛选**：支持从经验池中提取 top-k 高奖励分子，用于分析或进一步训练。

---

## 5. 代码模块对应关系

- `generator.py`  
  - `GenSampler`：采样、经验回放、MC Dropout 不确定性估计  
  - `RewardBuffer`：奖励归一化、经验池管理、采样概率分配

---

## 总结

RUMC 通过奖励归一化、经验池智能回放和 MC Dropout 不确定性采样，显著提升了生成模型的探索能力、分子多样性和奖励驱动优化效果。这些机制协同工作，有效促进高质量分子的发现和生成。